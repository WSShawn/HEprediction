{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9580,"status":"ok","timestamp":1672490038552,"user":{"displayName":"Tuan Tran","userId":"13247013337371011924"},"user_tz":300},"id":"7D-ClLnDbFpR","outputId":"6cf01189-4415-4264-865e-ee39ed00c3a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"drive/My Drive/HEprediction\")\n","!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n","import monai\n","from monai.data import  DataLoader, ImageDataset\n","from monai.transforms import (\n","    Resize, NormalizeIntensity, Activations, Compose, EnsureType, CenterSpatialCrop,ScaleIntensity,ResizeWithPadOrCrop, ResizeWithPadOrCropd,\n","    LoadImaged, EnsureChannelFirstd, EnsureTyped, NormalizeIntensityd, ScaleIntensityd,AddChannel\n",")\n","from monai.data import CacheDataset, DataLoader, ImageDataset, Dataset\n","from monai.data import decollate_batch\n","from monai.networks.nets import DenseNet121\n","from skimage.morphology import disk, binary_dilation, binary_erosion, remove_small_objects\n","import pandas as pd\n","import numpy as np\n","import nibabel as nib\n","import torch\n","import scipy.ndimage as nd\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import nibabel.processing\n","from monai.utils import set_determinism\n","from MyDenseNet import MyDenseNet121\n","import random\n","random.seed(123)\n","set_determinism(seed=123)\n","from monai.metrics import DiceMetric\n","from monai.networks.nets import SegResNet\n","from monai.inferers import sliding_window_inference\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    ConfusionMatrixDisplay\n",")\n","VAL_AMP = True\n","def inference(input):\n","    def _compute(input):\n","        return sliding_window_inference(\n","            inputs=input,\n","            roi_size=(256, 256, 32),\n","            sw_batch_size=4,\n","            predictor=model,\n","            overlap=0.5,\n","        )\n","\n","    if VAL_AMP:\n","        with torch.cuda.amp.autocast():\n","            return _compute(input)\n","    else:\n","        return _compute(input)\n","from monai.transforms import (\n","    Activations, AsDiscrete,\n","    Compose\n",")\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def plot_slices(num_rows, num_columns, width, height, data):\n","    data = np.transpose(data)\n","    data = np.expand_dims(data, axis=0)\n","    data = np.reshape(data, (num_rows, num_columns, width, height))\n","    rows_data, columns_data = data.shape[0], data.shape[1]\n","    heights = [slc[0].shape[0] for slc in data]\n","    widths = [slc.shape[1] for slc in data[0]]\n","    fig_width = 12.0\n","    fig_height = fig_width * sum(heights) / sum(widths)\n","    f, axarr = plt.subplots(\n","        rows_data,\n","        columns_data,\n","        figsize=(fig_width, fig_height),\n","        gridspec_kw={\"height_ratios\": heights},\n","    )\n","    for i in range(rows_data):\n","        for j in range(columns_data):\n","            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n","            axarr[i, j].axis(\"off\")\n","    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n","    plt.show()"]},{"cell_type":"code","source":["#Preprocessing for segmentation: input: CT, groundtruth; output:brain, groundtruth (512x512x48)\n","data_dir = \"italy_ds/\"\n","output_path=data_dir + \"preprocessing\"\n","raw_data = pd.read_csv(os.path.join(data_dir, \"labels.csv\"), sep=\";\")\n","for _, c_row in raw_data.iterrows():\n","    patient = str(c_row['filename'])\n","    pathCT1 = os.path.join(data_dir, \"baseline\", patient + \".nii.gz\")\n","    pathCT1seg = os.path.join(data_dir, \"mask\", patient + \"-label.nii.gz\")\n","    # load patient and extract brain window\n","    nii = nib.load(pathCT1seg)\n","    tmpimg1seg = np.round(nii.get_fdata()).astype(int)\n","    tmpimg1seg[tmpimg1seg != 1] = 0\n","    window_center, window_width = 40, 80\n","    nii = nib.load(pathCT1)\n","    tmpimg1 = nii.get_fdata()\n","    tmpimg1[tmpimg1 < 0] = 0\n","    tmpimg1[tmpimg1 > 200] = 0\n","    img_min = window_center - window_width // 2\n","    img_max = window_center + window_width // 2\n","    tmpimg1[tmpimg1 < img_min] = img_min\n","    tmpimg1[tmpimg1 > img_max] = img_max\n","    tmpimg1 = (tmpimg1 - tmpimg1.min()) / np.ptp(tmpimg1)\n","\n","    img1 = tmpimg1\n","    img1seg = tmpimg1seg\n","\n","    img1 = 1.0 * (img1 - img1.min()) / np.ptp(img1)\n","    img_bw = img1.copy()\n","    img_bw[img_bw > 0] = 1\n","    for slice in range(0, img_bw.shape[2]):\n","        if slice <= 2 or slice > img_bw.shape[2] - 2:\n","            img_bw[:, :, slice] = 0\n","        if img_bw[:, :, slice].sum() > 0:\n","            img_bw[:, :, slice] = binary_erosion(img_bw[:, :, slice].astype(np.uint8),\n","                                                 disk(4, dtype=bool))\n","            img_bw[:, :, slice] = remove_small_objects(img_bw[:, :, slice].astype(bool), 1000)\n","            img_bw[:, :, slice] = binary_dilation(img_bw[:, :, slice].astype(np.uint8),\n","                                                  disk(4, dtype=bool))\n","            img_bw[:, :, slice] = nd.binary_fill_holes(img_bw[:, :, slice].astype(np.uint8))\n","    img1[img_bw == 0] = 0\n","    img1 = 1.0 * (img1 - img1.min()) / np.ptp(img1)\n","    resizeImage = ResizeWithPadOrCrop(spatial_size=(512, 512, 48))\n","    scaleImage = ScaleIntensity()\n","    if (img1.shape[0] != img1seg.shape[0] or img1.shape[1] != img1seg.shape[1] or img1.shape[2] != img1seg.shape[2]):\n","        img1_2 = np.zeros([1, img1.shape[0], img1.shape[1], img1.shape[2]])\n","        img1_2[0, :, :, :] = img1\n","        resizeSeg = ResizeWithPadOrCrop(spatial_size=(img1seg.shape[0], img1seg.shape[1], img1seg.shape[2]))\n","        img1_2 = resizeSeg(img1_2)\n","        image = np.zeros([2, img1seg.shape[0], img1seg.shape[1], img1seg.shape[2]])\n","        image[0:1, :, :, :] = img1_2\n","        image[1, :, :, :] = img1seg\n","        image = resizeImage(image)\n","    else:\n","        image = np.zeros([2, img1seg.shape[0], img1seg.shape[1], img1seg.shape[2]])\n","        image[0, :, :, :] = img1\n","        image[1, :, :, :] = img1seg\n","        image = resizeImage(image)\n","\n","    empty_header = nib.Nifti1Header()\n","    nii = nib.load(pathCT1)\n","    clipped_img = nib.Nifti1Image(image[0, :, :, :], nii.affine, empty_header)\n","    pathCT1 = os.path.join(output_path, patient + \"_brain.nii.gz\")\n","    nib.save(clipped_img, pathCT1)\n","\n","    nii = nib.load(pathCT1seg)\n","    dim = (nii.header[\"pixdim\"])[1:4]\n","    clipped_img = nib.Nifti1Image(image[1, :, :, :], nii.affine, empty_header)\n","    pathCT1seg = os.path.join(output_path, patient + \"_ich_seg.nii.gz\")\n","    nib.save(clipped_img, pathCT1seg)\n"],"metadata":{"id":"a7h-dJWUS6Ah","executionInfo":{"status":"ok","timestamp":1672490123239,"user_tz":300,"elapsed":22306,"user":{"displayName":"Tuan Tran","userId":"13247013337371011924"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVJ9_n8OZSlq","outputId":"4ac7a3b7-8ecb-496d-bb19-438aeebd3cec","executionInfo":{"status":"ok","timestamp":1672490547243,"user_tz":300,"elapsed":12490,"user":{"displayName":"Tuan Tran","userId":"13247013337371011924"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0:10028\n","1:10042\n","2:10051\n","3:10052\n","4:10056\n","5:E106342615\n"]}],"source":["# segmentation: input: brain; output: seg_auto\n","data_dir = \"italy_ds/\"\n","output_path=data_dir + \"preprocessing\"\n","raw_data = pd.read_csv(os.path.join(data_dir, \"labels.csv\"), sep=\";\")\n","xtest = []\n","ytest = []\n","nametest = []\n","for _, c_row in raw_data.iterrows():\n","    xtest = np.append(xtest, os.path.join(output_path, str(c_row['filename']) + \"_brain.nii.gz\"))\n","    ytest = np.append(ytest,\n","                      os.path.join(output_path, str(c_row['filename']) + \"_ich_seg.nii.gz\"))\n","    nametest = np.append(nametest, str(c_row['filename']))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","test_transforms = Compose(\n","    [\n","        LoadImaged(keys=[\"image\", \"groundtruth\"]),\n","        EnsureChannelFirstd(keys=[\"image\", \"groundtruth\"]),\n","        EnsureTyped(keys=[\"image\", \"groundtruth\"]),\n","        NormalizeIntensityd(keys=\"image\"),\n","        ScaleIntensityd(keys=\"image\"),\n","    ]\n",")\n","test_files = [{\"image\": t2Img, \"groundtruth\": adcImg, \"name\": name} for t2Img, adcImg, name in\n","              zip(xtest, ytest, nametest)]\n","test_ds = Dataset(data=test_files, transform=test_transforms)\n","test_loader = DataLoader(test_ds, batch_size=1, shuffle=False,\n","                         num_workers=1, pin_memory=torch.cuda.is_available())\n","model = SegResNet(\n","    blocks_down=[1, 2, 2, 4],\n","    blocks_up=[1, 1, 1],\n","    init_filters=16,\n","    in_channels=1,\n","    out_channels=1,\n","    dropout_prob=0.2,\n",").to(device)\n","model.load_state_dict(\n","    torch.load(os.path.join(\"1_6000_new_best_metric_model_segmentation3d_1_1.pth\")))\n","\n","post_trans = Compose(\n","    [Activations(sigmoid=True), AsDiscrete(threshold=0.5)]\n",")\n","\n","model.eval()\n","with torch.no_grad():\n","    num = 0\n","    for val_data in test_loader:\n","        val_inputs, val_labels = (\n","            val_data[\"image\"].to(device),\n","            val_data[\"groundtruth\"].to(device),\n","        )\n","        print(str(num) + \":\" + str(val_data[\"name\"][0]))\n","        num = num + 1\n","        val_outputs = inference(val_inputs)\n","        val_outputs = post_trans(val_outputs)\n","        t1 = val_outputs[0, 0, :, :, :].cpu().numpy()\n","        nii = nib.load(os.path.join(output_path, str(val_data[\"name\"][0]) + \"_\" + \"ich_seg.nii.gz\"))\n","        empty_header = nib.Nifti1Header()\n","        affine = np.eye(4)\n","        affine[:3, :3] = np.diag((nii.header[\"pixdim\"])[1:4])\n","        pathCT1seg = os.path.join(output_path, str(val_data[\"name\"][0]) + \"_seg_auto.nii.gz\")\n","        imgseg = nib.Nifti1Image(t1, affine, empty_header)\n","        nib.save(imgseg, pathCT1seg)"]},{"cell_type":"code","source":["# preprocessing for classification\n","data_dir = \"italy_ds/\"\n","output_path = data_dir + \"preprocessing\"\n","raw_data = pd.read_csv(os.path.join(data_dir, \"labels.csv\"), sep=\";\")\n","xtest = []\n","ytest = []\n","nametest = []\n","for _, c_row in raw_data.iterrows():\n","    patient = str(c_row['filename'])\n","    pathCT1 = os.path.join(data_dir, \"baseline\", patient + \".nii.gz\")\n","    pathCT1seg = os.path.join(output_path, str(c_row['filename']) + \"_seg_auto.nii.gz\")\n","    nii = nib.load(pathCT1seg)\n","    print(\"Original\")\n","    print(nii.get_fdata().shape)\n","    tmp = (nii.header[\"pixdim\"])[1:4]\n","    print(nii.header[\"pixdim\"])\n","    image = nii.get_fdata()\n","\n","    voxel_size = [1, 1, 1]\n","    tmpimg1seg = np.round(nii.get_fdata()).astype(int)\n","    tmpimg1seg[tmpimg1seg != 1] = 0\n","\n","    affine = np.eye(4)\n","    affine[:3, :3] = np.diag((nii.header[\"pixdim\"])[1:4])\n","    pathtmp = os.path.join(data_dir, \"1tmp_brain_seg.nii.gz\")\n","    empty_header = nib.Nifti1Header()\n","    clipped_img = nib.Nifti1Image(tmpimg1seg, affine, empty_header)\n","    nib.save(clipped_img, pathtmp)\n","    nii = nib.load(pathtmp)\n","    nii = nibabel.processing.resample_to_output(nii, voxel_size)\n","    tmpimg1seg = np.round(nii.get_fdata()).astype(int)\n","    tmpimg1seg[tmpimg1seg > 0] = 1\n","\n","    # load bone window and resample\n","    window_center, window_width = 40, 80\n","    nii = nib.load(pathCT1)\n","    tmpimg1 = nii.get_fdata()\n","    tmpimg1[tmpimg1 < 0] = 0\n","    tmpimg1[tmpimg1 > 200] = 0\n","    resizeImage = ResizeWithPadOrCrop(spatial_size=(512, 512, 48))\n","    addChannel = AddChannel()\n","    image = resizeImage(addChannel(tmpimg1))\n","    tmpimg1 = image[0, :, :, :]\n","\n","    pathtmp = os.path.join(data_dir, \"3tmp_brain_seg.nii.gz\")\n","    empty_header = nib.Nifti1Header()\n","    clipped_img = nib.Nifti1Image(tmpimg1, affine, empty_header)\n","    nib.save(clipped_img, pathtmp)\n","    nii = nib.load(pathtmp)\n","    nii = nibabel.processing.resample_to_output(nii, voxel_size)\n","    tmpimg1 = nii.get_fdata()\n","    img_min = window_center - window_width // 2\n","    img_max = window_center + window_width // 2\n","    tmpimg1[tmpimg1 < img_min] = img_min\n","    tmpimg1[tmpimg1 > img_max] = img_max\n","    tmpimg1 = (tmpimg1 - tmpimg1.min()) / np.ptp(tmpimg1)\n","    img_bw = tmpimg1.copy()\n","    img_bw[img_bw > 0] = 1\n","    for slice in range(0, img_bw.shape[2]):\n","        if slice <= 2 or slice > img_bw.shape[2] - 2:\n","            img_bw[:, :, slice] = 0\n","        if img_bw[:, :, slice].sum() > 0:\n","            img_bw[:, :, slice] = binary_erosion(img_bw[:, :, slice].astype(np.uint8),\n","                                                 disk(4, dtype=bool))\n","            img_bw[:, :, slice] = remove_small_objects(img_bw[:, :, slice].astype(bool), 1000)\n","            img_bw[:, :, slice] = binary_dilation(img_bw[:, :, slice].astype(np.uint8),\n","                                                  disk(4, dtype=bool))\n","            img_bw[:, :, slice] = nd.binary_fill_holes(img_bw[:, :, slice].astype(np.uint8))\n","    tmpimg1[img_bw == 0] = 0\n","    tmpimg1 = 1.0 * (tmpimg1 - tmpimg1.min()) / np.ptp(tmpimg1)\n","\n","    # find the first slice which contains hematoma\n","    for slice1 in range(tmpimg1seg.shape[2]):\n","        if tmpimg1seg[:, :, slice1].sum() > 0:\n","            break\n","    # find the last slice which contains hematoma\n","    for slice2 in range(tmpimg1seg.shape[2] - 1, 0, -1):\n","        if tmpimg1seg[:, :, slice2].sum() > 0:\n","            break\n","    if slice1 < slice2:\n","        slice1 = slice1 - 16\n","        if slice1 < 0:\n","            slice1 = 0\n","        slice2 = slice2 + 16\n","        if slice2 > tmpimg1seg.shape[2] - 1:\n","            slice2 = tmpimg1seg.shape[2] - 1\n","        if slice2 - slice1 >= 96:\n","            slice2 = slice1 + 96\n","        img1 = np.zeros([tmpimg1.shape[0], tmpimg1.shape[1], 96])\n","        img1seg = np.zeros([tmpimg1seg.shape[0], tmpimg1seg.shape[1], 96])\n","        img1[:, :, 0:slice2 - slice1] = tmpimg1[:, :, slice1:slice2]\n","        img1seg[:, :, 0:slice2 - slice1] = tmpimg1seg[:, :, slice1:slice2]\n","    else:\n","        img1 = np.zeros([tmpimg1.shape[0], tmpimg1.shape[1], 96])\n","        img1[:, :, 0:64] = tmpimg1[:, :,\n","                           int(np.round(tmpimg1.shape[2] / 2) - 32):int(np.round(tmpimg1.shape[2] / 2) + 32)]\n","        img1seg = np.zeros([tmpimg1seg.shape[0], tmpimg1seg.shape[1], 96])\n","\n","    img1 = 1.0 * (img1 - img1.min()) / np.ptp(img1)\n","\n","    img1dilation = img1.copy()\n","    img_bw = img1seg.copy()\n","    for slice in range(0, img_bw.shape[2]):\n","        if img_bw[:, :, slice].sum() > 0:\n","            img_bw[:, :, slice] = binary_dilation(img_bw[:, :, slice].astype(np.uint8),\n","                                                  disk(40, dtype=bool))\n","    img1dilation[img_bw == 0] = 0\n","\n","    imgfinal = np.zeros([2, img1.shape[0], img1.shape[1], img1.shape[2]])\n","    imgfinal[0, :, :, :] = img1\n","    imgfinal[1, :, :, :] = img1dilation\n","    image = imgfinal\n","\n","    # save file\n","    pathCT1clipseg = os.path.join(data_dir, \"classification\", patient + \"_brain_seg_dilation.nii.gz\")\n","    empty_header = nib.Nifti1Header()\n","    nii = nib.load(pathCT1)\n","    clipped_img = nib.Nifti1Image(imgfinal, nii.affine, empty_header)\n","    nib.save(clipped_img, pathCT1clipseg)\n","\n","    imgfinal = np.zeros([2, img1.shape[0], img1.shape[1], img1.shape[2]])\n","    imgfinal[0, :, :, :] = img1\n","    imgfinal[1, :, :, :] = img1seg\n","    image = imgfinal\n","\n","    # save file\n","    pathCT1clipseg = os.path.join(data_dir, \"classification\", patient + \"_brain_seg.nii.gz\")\n","    clipped_img = nib.Nifti1Image(imgfinal, nii.affine, empty_header)\n","    nib.save(clipped_img, pathCT1clipseg)\n"],"metadata":{"id":"RAy9P3MnykwL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672490439344,"user_tz":300,"elapsed":218354,"user":{"displayName":"Tuan Tran","userId":"13247013337371011924"}},"outputId":"5e40e679-6dce-44ff-a8b5-c58faf741a80"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Original\n","(512, 512, 48)\n","[1.      0.46875 0.46875 5.      1.      1.      1.      1.     ]\n","Original\n","(512, 512, 48)\n","[1.        0.488281  0.5701952 4.5675735 1.        1.        1.\n"," 1.       ]\n","Original\n","(512, 512, 48)\n","[1.        0.468     0.468     5.9917765 1.        1.        1.\n"," 1.       ]\n","Original\n","(512, 512, 48)\n","[1.         0.48828125 0.48828125 3.         1.         1.\n"," 1.         1.        ]\n","Original\n","(512, 512, 48)\n","[1.       0.488281 0.488281 5.       1.       1.       1.       1.      ]\n","Original\n","(512, 512, 48)\n","[1.         0.474609   0.47460908 3.682628   1.         1.\n"," 1.         1.        ]\n"]}]},{"cell_type":"code","source":["root_dir = \"italy_ds/\"\n","data_dir = os.path.join(root_dir, \"classification\")\n","raw_data = pd.read_csv(os.path.join(root_dir, \"labels.csv\"), sep=\";\")\n","pathFilenames = []\n","labels = []\n","names = []\n","for _, c_row in raw_data.iterrows():\n","    pathFilenames.append(os.path.join(data_dir, str(c_row['filename']) + \"_brain_seg.nii.gz\"))\n","    labels.append(c_row['label'])\n","    names.append(str(c_row['filename']))\n","labels = np.asarray(labels).astype(int)\n","pathFilenames = np.asarray(pathFilenames)\n","names = np.asarray(names)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = MyDenseNet121(spatial_dims=3, in_channels=2, out_channels=128).to(device)\n","model.cuda()\n","model.load_state_dict(torch.load(\"96dilation_test_best_metric_model_classification3d_11.pth\"))\n","model.eval()\n","xval = pathFilenames\n","yval = labels\n","val_ds = ImageDataset(\n","    image_files=xval, labels=yval)\n","val_loader = DataLoader(val_ds, batch_size=1, shuffle=False,\n","                        num_workers=1, pin_memory=torch.cuda.is_available())\n","y_pred_trans = Compose([EnsureType(), Activations(sigmoid=True)])\n","y_trans = Compose([EnsureType()])\n","with torch.no_grad():\n","    y_pred = torch.tensor([], dtype=torch.float32, device=device)\n","    y = torch.tensor([], dtype=torch.double, device=device)\n","    for val_data in val_loader:\n","        inputs, val_labels = val_data[0].to(\n","            device), val_data[1].to(device)\n","        val_outputs = model(inputs)\n","        y_pred1 = val_outputs.flatten()\n","        y1 = val_labels\n","        y_pred = torch.cat([y_pred, y_pred1], dim=0)\n","        y = torch.cat([y, y1], dim=0)\n","    y_onehot = [y_trans(i) for i in decollate_batch(y)]\n","    y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n","    auc_metric1 = monai.metrics.ROCAUCMetric()\n","    auc_metric1(y_pred_act, y_onehot)\n","    print(\"AUC total= \")\n","    print(auc_metric1.aggregate())\n","    auc_metric1.reset()\n"],"metadata":{"id":"4Up055RkynrM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672490621478,"user_tz":300,"elapsed":3122,"user":{"displayName":"Tuan Tran","userId":"13247013337371011924"}},"outputId":"052f2524-f0a3-4f06-ed99-cc9cd390fe54"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC total= \n","0.7777777777777778\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1WSmJAGEuG1UKaw2gijvNeKBDFAIgsCxI","timestamp":1648200209441}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}